{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIP Framework training example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/WORKS/UBC/RecoBips/BIPs.jl`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")\n",
    "using BIPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using Pkg.Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by bringing in the dataset. It contains tree splits:\n",
    "* **train**: the training set with 1M jets\n",
    "* **validation**: the validation set with 400k jets\n",
    "\n",
    "And of course later we will use the **test** set with other 400k jets to report the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/jose/Documents/WORKS/UBC/Datalake/reco_ds\"\n",
    "\n",
    "train_data_path = dataset_path*\"/train.h5\"\n",
    "val_data_path = dataset_path*\"/valid.h5\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "\n",
    "In order to read the datasets, we call the `read_dataset` function:\n",
    "to read the TopQuark format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length(targets[1, :]) = 686539\n",
      "Number of entries in the training data: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686539"
     ]
    }
   ],
   "source": [
    "train_jets, train_labels = BIPs.read_data(\"RecoTQ\", train_data_path)\n",
    "print(\"Number of entries in the training data: \", length(train_jets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length(targets[1, :]) = 196153\n",
      "Number of entries in the validation data: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196153"
     ]
    }
   ],
   "source": [
    "val_jets, val_labels = BIPs.read_data(\"RecoTQ\", val_data_path)\n",
    "print(\"Number of entries in the validation data: \", length(val_jets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2389.734145552389"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_scale = maximum(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine how one of the jets looks like, each one of the entries is one detected particle's four momentum $(E, p_x, p_y, p_z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However,in order to compute the embeddings, it is necesary to convert the jets to a format that can be used by the framework. The function `data2hyp` allows to convert each detected four momentum to the jet basis, a.k.a $(\\tilde p_T, \\cos(\\theta), \\sin(\\theta), \\tilde y, E_T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed jets\n"
     ]
    }
   ],
   "source": [
    "train_transf_jets = data2basis(train_jets; basis=\"hyp\")\n",
    "val_transf_jets = data2basis(val_jets; basis=\"hyp\")\n",
    "println(\"Transformed jets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the jets are converted to the jet basis, it is moment to embed the model using the *Invariant Polynomials*. \n",
    "\n",
    "The function `build_ip` allocates efficiently the sparse basis, while the `bip_data` computes the invariant representation of each one of the jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bip_data (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_bip, specs, a_basis = build_ip(order=2, levels=5)\n",
    "    \n",
    "function bip_data(dataset_jets)\n",
    "    storage = zeros(length(dataset_jets), length(specs))\n",
    "    for i = 1:length(dataset_jets)\n",
    "        storage[i, :] = f_bip(dataset_jets[i])\n",
    "    end\n",
    "    storage[:, 2:end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded train jets correclty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded test jets correclty\n"
     ]
    }
   ],
   "source": [
    "train_embedded_jets = bip_data(train_transf_jets)\n",
    "println(\"Embedded train jets correclty\")\n",
    "val_embedded_jets = bip_data(val_transf_jets)\n",
    "println(\"Embedded test jets correclty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_jets = train_embedded_jets/maximum(train_embedded_jets)\n",
    "scaled_val_jets = val_embedded_jets/maximum(train_embedded_jets)\n",
    "\n",
    "scaled_train_labels = train_labels/target_scale\n",
    "scaled_val_labels = val_labels/target_scale;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings are now created for the dataset. From this point on, the classification itself is absolutelly versatile. For this specific example we will use the out-of-the box classifier `sklearn.linear_model.HistGradientBoostingClassifier` that bines the data and then applies a grandient boosted trees algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets fit a simple model to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "@pyimport sklearn.ensemble as sk_ensemble\n",
    "@pyimport sklearn.metrics as sk_metrics\n",
    "@pyimport sklearn.neural_network as sk_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree, 31 leaves, max depth = 12, train loss: 44494.07774, val loss: 44704.62935, in 0.071s\n",
      "Fit 10 trees in 8.597 s, (310 total leaves)\n",
      "Time spent computing histograms: 0.389s\n",
      "Time spent finding best splits:  0.035s\n",
      "Time spent applying splits:      0.054s\n",
      "Time spent predicting:           0.012s\n",
      "Binning 0.247 GB of training data: 1.546 s\n",
      "Binning 0.027 GB of validation data: 0.035 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 31 leaves, max depth = 11, train loss: 0.00782, val loss: 0.00761, in 0.062s\n",
      "[2/100] 1 tree, 31 leaves, max depth = 11, train loss: 0.00782, val loss: 0.00761, in 0.044s\n",
      "[3/100] 1 tree, 31 leaves, max depth = 13, train loss: 0.00782, val loss: 0.00761, in 0.058s\n",
      "[4/100] 1 tree, 31 leaves, max depth = 8, train loss: 0.00782, val loss: 0.00761, in 0.048s\n",
      "[5/100] 1 tree, 31 leaves, max depth = 15, train loss: 0.00782, val loss: 0.00761, in 0.050s\n",
      "[6/100] 1 tree, 31 leaves, max depth = 11, train loss: 0.00782, val loss: 0.00761, in 0.042s\n",
      "[7/100] 1 tree, 31 leaves, max depth = 12, train loss: 0.00782, val loss: 0.00761, in 0.049s\n",
      "[8/100] 1 tree, 31 leaves, max depth = 10, train loss: 0.00782, val loss: 0.00761, in 0.039s\n",
      "[9/100] 1 tree, 31 leaves, max depth = 11, train loss: 0.00782, val loss: 0.00761, in 0.039s\n",
      "[10/100] 1 tree, 31 leaves, max depth = 14, train loss: 0.00782, val loss: 0.00761, in 0.032s\n",
      "[11/100] "
     ]
    }
   ],
   "source": [
    "GCT = sk_ensemble.HistGradientBoostingRegressor(verbose=true).fit(train_embedded_jets, scaled_train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCT.predict(val_embedded_jets);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lest test how we do performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understanad the framework, lets see how our model performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/jose/Documents/WORKS/UBC/Datalake/reco_ds/test.h5\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data_path = dataset_path*\"/test.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length(targets[1, :]) = 98081\n",
      "Embedded test jets correclty"
     ]
    }
   ],
   "source": [
    "test_jets, test_labels = BIPs.read_data(\"RecoTQ\", test_data_path)\n",
    "test_transf_jets = data2basis(test_jets; basis=\"hyp\")\n",
    "test_embedded_jets = bip_data(test_transf_jets)\n",
    "print(\"Embedded test jets correclty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = GCT.score(test_embedded_jets, test_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "writedlm( \"/home/jose/Documents/WORKS/UBC/RecoBips/saved_basis/train_basis.csv\",  train_embedded_jets, ',')\n",
    "writedlm( \"/home/jose/Documents/WORKS/UBC/RecoBips/saved_basis/train_labels.csv\",  train_labels, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "writedlm( \"/home/jose/Documents/WORKS/UBC/RecoBips/saved_basis/val_basis.csv\",  val_embedded_jets, ',')\n",
    "writedlm( \"/home/jose/Documents/WORKS/UBC/RecoBips/saved_basis/val_labels.csv\",  val_labels, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "writedlm( \"/home/jose/Documents/WORKS/UBC/RecoBips/saved_basis/test_basis.csv\",  test_embedded_jets, ',')\n",
    "writedlm( \"/home/jose/Documents/WORKS/UBC/RecoBips/saved_basis/test_labels.csv\",  test_labels, ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed1560bd249103eb874556eada0ab548e2eb318ac2260e7986d2cded9b278337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
